"""
Transformer Demo - Quick Start Guide
====================================

This file provides a quick demonstration of the Transformer implementation
and shows key features for the assignment deliverables.
"""

# Import our complete implementation
from transformer_numpy import (
    TokenEmbedding, PositionalEncoding, LayerNormalization,
    ScaledDotProductAttention, MultiHeadAttention, FeedForwardNetwork, 
    TransformerBlock, GPTTransformer, create_causal_mask, softmax, gelu
)
import numpy as np

def demonstrate_transformer():
    """Comprehensive demonstration of the Transformer implementation."""
    
    print("ğŸ¯ TRANSFORMER ARCHITECTURE DEMONSTRATION")
    print("=" * 50)
    
    # Configuration
    config = {
        'vocab_size': 1000,
        'embed_dim': 256,  # Smaller for demo
        'num_heads': 8,
        'num_layers': 4,   # Smaller for demo  
        'ff_dim': 1024,    # 4 * embed_dim
        'max_seq_len': 64
    }
    
    batch_size = 2
    seq_len = 12
    
    print("ğŸ“‹ Model Configuration:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    print()
    
    # 1. COMPONENT TESTING
    print("ğŸ”§ TESTING INDIVIDUAL COMPONENTS")
    print("-" * 30)
    
    # Test Token Embedding
    print("1ï¸âƒ£ Token Embedding:")
    token_emb = TokenEmbedding(config['vocab_size'], config['embed_dim'])
    token_ids = np.random.randint(0, config['vocab_size'], (batch_size, seq_len))
    embeddings = token_emb.forward(token_ids)
    print(f"   Input: {token_ids.shape} -> Output: {embeddings.shape}")
    
    # Test Positional Encoding  
    print("2ï¸âƒ£ Positional Encoding:")
    pos_enc = PositionalEncoding(config['max_seq_len'], config['embed_dim'])
    pos_embeddings = pos_enc.forward(embeddings)
    print(f"   Added positional info: {pos_embeddings.shape}")
    
    # Test Causal Mask
    print("3ï¸âƒ£ Causal Mask:")
    mask = create_causal_mask(seq_len)
    print(f"   Mask shape: {mask.shape}")
    print(f"   Lower triangular: {np.array_equal(mask, np.tril(np.ones((seq_len, seq_len))).astype(bool))}")
    
    # Test Attention
    print("4ï¸âƒ£ Scaled Dot-Product Attention:")
    attention = ScaledDotProductAttention(config['embed_dim'])
    attn_out, attn_weights = attention.forward(pos_embeddings, pos_embeddings, pos_embeddings, mask)
    print(f"   Attention output: {attn_out.shape}")
    print(f"   Attention weights: {attn_weights.shape}")
    print(f"   Weights sum to 1: {np.allclose(np.sum(attn_weights, axis=-1), 1.0)}")
    
    # Test Multi-Head Attention
    print("5ï¸âƒ£ Multi-Head Attention:")
    mha = MultiHeadAttention(config['embed_dim'], config['num_heads'])
    mha_out, mha_weights = mha.forward(pos_embeddings, pos_embeddings, pos_embeddings, mask)
    print(f"   MHA output: {mha_out.shape}")
    print(f"   MHA weights: {mha_weights.shape}")
    
    # Test Feed-Forward Network
    print("6ï¸âƒ£ Feed-Forward Network:")
    ffn = FeedForwardNetwork(config['embed_dim'], config['ff_dim'])
    ffn_out = ffn.forward(pos_embeddings)
    print(f"   FFN output: {ffn_out.shape}")
    
    # Test Layer Normalization
    print("7ï¸âƒ£ Layer Normalization:")
    layer_norm = LayerNormalization(config['embed_dim'])
    norm_out = layer_norm.forward(pos_embeddings)
    mean_check = np.mean(norm_out, axis=-1)
    var_check = np.var(norm_out, axis=-1)
    print(f"   Mean â‰ˆ 0: {np.allclose(mean_check, 0, atol=1e-6)}")
    print(f"   Variance â‰ˆ 1: {np.allclose(var_check, 1, atol=1e-6)}")
    
    print()
    
    # 2. COMPLETE MODEL TESTING
    print("ğŸ—ï¸ COMPLETE TRANSFORMER MODEL")
    print("-" * 30)
    
    # Initialize full model
    model = GPTTransformer(**config)
    print("âœ… Model initialized successfully!")
    
    # Forward pass
    logits, attention_weights = model.forward(token_ids, return_attention=True)
    print(f"ğŸ“¤ Forward Pass Results:")
    print(f"   Input tokens: {token_ids.shape}")
    print(f"   Output logits: {logits.shape}")
    print(f"   Attention layers: {len(attention_weights)}")
    
    # Next token prediction
    next_token_probs = model.predict_next_token(token_ids)
    print(f"ğŸ¯ Next Token Prediction:")
    print(f"   Probabilities shape: {next_token_probs.shape}")
    print(f"   Probability sums: {np.sum(next_token_probs, axis=-1)}")
    print(f"   Max probability: {np.max(next_token_probs):.4f}")
    
    # Text generation
    prompt = token_ids[:1, :3]  # First 3 tokens
    generated = model.generate_text(prompt, max_new_tokens=8)
    print(f"ğŸ“ Text Generation:")
    print(f"   Prompt: {prompt[0]}")
    print(f"   Generated: {generated[0]}")
    
    print()
    
    # 3. MATHEMATICAL VERIFICATION
    print("ğŸ”¬ MATHEMATICAL VERIFICATION")
    print("-" * 30)
    
    # Verify softmax properties
    test_logits = np.random.randn(5, 10)
    probs = softmax(test_logits, axis=-1)
    print(f"âœ… Softmax sums to 1: {np.allclose(np.sum(probs, axis=-1), 1.0)}")
    
    # Verify causal masking
    scores = np.random.randn(seq_len, seq_len)
    masked_scores = np.where(mask, scores, -np.inf)
    causal_check = np.all(masked_scores[np.triu_indices(seq_len, k=1)] == -np.inf)
    print(f"âœ… Causal mask blocks future: {causal_check}")
    
    # Verify attention weights
    sample_attention = attention_weights[0][0, 0, :, :]  # First layer, first head
    attn_sum_check = np.allclose(np.sum(sample_attention, axis=-1), 1.0)
    print(f"âœ… Attention weights sum to 1: {attn_sum_check}")
    
    # Verify layer norm statistics
    sample_data = np.random.randn(100, config['embed_dim']) * 5 + 2
    normed = layer_norm.forward(sample_data)
    ln_mean = np.mean(normed, axis=-1)
    ln_var = np.var(normed, axis=-1)
    print(f"âœ… LayerNorm mean â‰ˆ 0: {np.allclose(ln_mean, 0, atol=1e-5)}")
    print(f"âœ… LayerNorm var â‰ˆ 1: {np.allclose(ln_var, 1, atol=1e-5)}")
    
    print()
    
    # 4. ARCHITECTURE ANALYSIS  
    print("ğŸ“Š ARCHITECTURE ANALYSIS")
    print("-" * 30)
    
    print("ğŸ—ï¸ Model Architecture:")
    print(f"   â€¢ {config['num_layers']} Transformer blocks")
    print(f"   â€¢ {config['num_heads']} attention heads per block") 
    print(f"   â€¢ {config['embed_dim']} embedding dimensions")
    print(f"   â€¢ {config['ff_dim']} feed-forward hidden size")
    print(f"   â€¢ {config['vocab_size']} vocabulary size")
    
    print("ğŸ”¢ Parameter Count Estimate:")
    embedding_params = config['vocab_size'] * config['embed_dim']
    attention_params_per_layer = 4 * config['embed_dim'] * config['embed_dim']  # Q,K,V,O
    ffn_params_per_layer = 2 * config['embed_dim'] * config['ff_dim'] + config['ff_dim'] + config['embed_dim']
    layer_norm_params_per_layer = 4 * config['embed_dim']  # 2 layer norms * 2 params each
    total_layer_params = config['num_layers'] * (attention_params_per_layer + ffn_params_per_layer + layer_norm_params_per_layer)
    output_params = config['embed_dim'] * config['vocab_size']
    
    total_params = embedding_params + total_layer_params + output_params
    print(f"   â€¢ Embedding: ~{embedding_params:,}")
    print(f"   â€¢ Transformer layers: ~{total_layer_params:,}")
    print(f"   â€¢ Output projection: ~{output_params:,}")
    print(f"   â€¢ Total: ~{total_params:,} parameters")
    
    print("ğŸ’¾ Memory Usage:")
    forward_memory = logits.nbytes + sum(w.nbytes for w in attention_weights)
    print(f"   â€¢ Forward pass: ~{forward_memory / 1024 / 1024:.2f} MB")
    
    print()
    
    # 5. DESIGN JUSTIFICATIONS
    print("ğŸ“ DESIGN JUSTIFICATIONS")  
    print("-" * 30)
    
    print("ğŸ”„ Positional Encoding Choice: Sinusoidal")
    print("   âœ“ No learned parameters needed")
    print("   âœ“ Can extrapolate to longer sequences")
    print("   âœ“ Captures both absolute and relative position")
    
    print("ğŸ­ Multi-Head Attention Benefits:")
    print("   âœ“ Captures different types of relationships")  
    print("   âœ“ Allows parallel processing")
    print("   âœ“ Reduces risk of attention collapse")
    
    print("ğŸ›¡ï¸ Causal Masking Purpose:")
    print("   âœ“ Prevents information leakage from future tokens")
    print("   âœ“ Maintains autoregressive property")
    print("   âœ“ Essential for decoder-only architecture")
    
    print("âš¡ Pre-Norm Architecture:")
    print("   âœ“ Better gradient flow")
    print("   âœ“ More stable training")
    print("   âœ“ Reduced need for warmup")
    
    print()
    print("ğŸ‰ DEMONSTRATION COMPLETED SUCCESSFULLY!")
    print("âœ… All components working correctly")
    print("âœ… Mathematical properties verified") 
    print("âœ… Architecture design justified")
    print("ğŸš€ Ready for submission!")

if __name__ == "__main__":
    demonstrate_transformer()