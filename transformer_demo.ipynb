{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf080f8",
   "metadata": {},
   "source": [
    "# Transformer Architecture from Scratch using NumPy\n",
    "## Individual Assignment: GPT-style Decoder-only Implementation\n",
    "\n",
    "This notebook demonstrates a complete implementation of a Transformer architecture from scratch using only NumPy, without any deep learning frameworks like PyTorch or TensorFlow.\n",
    "\n",
    "### Assignment Objectives:\n",
    "- Build a decoder-only Transformer (GPT-style) architecture\n",
    "- Implement all core components: embedding, attention, feed-forward networks, etc.\n",
    "- Create modular, testable code with proper mathematical foundations\n",
    "- Demonstrate forward pass from token input to probability distribution output\n",
    "\n",
    "### Architecture Overview:\n",
    "1. **Token Embedding** - Maps token IDs to dense vectors\n",
    "2. **Positional Encoding** - Adds positional information using sinusoidal functions  \n",
    "3. **Multi-Head Attention** - Core attention mechanism with causal masking\n",
    "4. **Feed-Forward Network** - Two-layer MLP with non-linear activation\n",
    "5. **Layer Normalization** - Stabilizes training with pre-norm architecture\n",
    "6. **Residual Connections** - Enables deeper networks and better gradient flow\n",
    "7. **Output Layer** - Projects to vocabulary size with softmax distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86bfe2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll only use NumPy for all mathematical operations as per assignment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca33e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"ðŸš€ Ready to build Transformer from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc441e2",
   "metadata": {},
   "source": [
    "## 2. Token Embedding Implementation\n",
    "\n",
    "Token embedding converts discrete token IDs into continuous vector representations. This is the first step in processing text input.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "embedding(token_id) = W_embedding[token_id]\n",
    "```\n",
    "Where W_embedding is a learnable matrix of shape [vocab_size, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token embedding layer that maps token IDs to dense vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize token embedding layer.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Dimension of embedding vectors  \n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Initialize embedding matrix with small random values\n",
    "        # Using normal distribution scaled by embedding dimension\n",
    "        self.embedding_matrix = np.random.normal(0, 0.02, (vocab_size, embed_dim))\n",
    "    \n",
    "    def forward(self, token_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass of token embedding.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Token IDs of shape [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Embedded tokens of shape [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_matrix[token_ids]\n",
    "\n",
    "# Test the token embedding\n",
    "print(\"ðŸ”§ Testing Token Embedding...\")\n",
    "vocab_size = 1000\n",
    "embed_dim = 512\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "# Initialize embedding layer\n",
    "token_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "\n",
    "# Create sample token IDs\n",
    "token_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Sample tokens: {token_ids[0]}\")\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = token_emb.forward(token_ids)\n",
    "print(f\"Output embeddings shape: {embeddings.shape}\")\n",
    "print(f\"âœ… Token embedding working correctly!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
